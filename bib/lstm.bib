@misc{www-keras-lstm,
    author = {{Keras}},
    title = {Working with RNNs},
    url = {https://keras.io/guides/working_with_rnns/},
    year = 2015,
}
 
@misc{www-keras-api,
    author = {{Keras}},
    title = {LSTM layer},
    url = {https://keras.io/api/layers/recurrent_layers/lstm/},
    month = jun,
    year = 2020
}

@misc{www-tensorflow,
    author = {tensorflow.org}},
    title = {TensorFlow},
    url = {https://www.tensorflow.org/},
    month = jun,
    year = 2020
}

@misc{www-pytorch,
    author = {{pytorch.org}},
    title = {PyTorch},
    url = {https://pytorch.org/},
    month = jun,
    year = 2020
}

@INCOLLECTION{Graves2009-qb,
   title     = "Offline Handwriting Recognition with Multidimensional Recurrent
                Neural Networks",
   booktitle = "Advances in Neural Information Processing Systems 21",
   author    = "Graves, Alex and Schmidhuber, J{\"u}rgen",
   editor    = "Koller, D and Schuurmans, D and Bengio, Y and Bottou, L",
   publisher = "Curran Associates, Inc.",
   pages     = "545--552",
   year      =  2009
 }
 
 @INPROCEEDINGS{Schmidhuber2005-oy,
   title     = "Evolino: Hybrid neuroevolution/optimal linear search for
                sequence prediction",
   booktitle = "Proceedings of the 19th International Joint Conferenceon
                Artificial Intelligence ({IJCAI})",
   author    = "Schmidhuber, J{\"u}rgen and Wierstra, Daan and Gomez, Faustino J",
   year      =  2005
 }
 
 @TECHREPORT{Hochreiter1991-mp,
   title       = "Untersuchungen zu dynamischen neuronalen Netzen",
   author      = "Hochreiter, S",
   number      = "Diploma thesis",
   institution = "Technische Univ. Munich, Institut f. Informatik",
   year        =  1991
 }
 
 
 @ARTICLE{Rumelhart1986-li,
   title    = "Learning representations by back-propagating errors",
   author   = "Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J",
   abstract = "We describe a new learning procedure, back-propagation, for
               networks of neurone-like units. The procedure repeatedly adjusts
               the weights of the connections in the network so as to minimize a
               measure of the difference between the actual output vector of the
               net and the desired output vector. As a result of the weight
               adjustments, internal `hidden' units which are not part of the
               input or output come to represent important features of the task
               domain, and the regularities in the task are captured by the
               interactions of these units. The ability to create useful new
               features distinguishes back-propagation from earlier, simpler
               methods such as the perceptron-convergence procedure1.",
   journal  = "Nature",
   volume   =  323,
   number   =  6088,
   pages    = "533--536",
   month    =  oct,
   year     =  1986
 }
  @ARTICLE{Hochreiter1997-dk,
   title    = "Long short-term memory",
   author   = "Hochreiter, S and Schmidhuber, J",
   abstract = "Learning to store information over extended time intervals by
               recurrent backpropagation takes a very long time, mostly because
               of insufficient, decaying error backflow. We briefly review
               Hochreiter's (1991) analysis of this problem, then address it by
               introducing a novel, efficient, gradient-based method called long
               short-term memory (LSTM). Truncating the gradient where this does
               not do harm, LSTM can learn to bridge minimal time lags in excess
               of 1000 discrete-time steps by enforcing constant error flow
               through constant error carousels within special units.
               Multiplicative gate units learn to open and close access to the
               constant error flow. LSTM is local in space and time; its
               computational complexity per time step and weight is O(1). Our
               experiments with artificial data involve local, distributed,
               real-valued, and noisy pattern representations. In comparisons
               with real-time recurrent learning, back propagation through time,
               recurrent cascade correlation, Elman nets, and neural sequence
               chunking, LSTM leads to many more successful runs, and learns
               much faster. LSTM also solves complex, artificial long-time-lag
               tasks that have never been solved by previous recurrent network
               algorithms.",
   journal  = "Neural Comput.",
   volume   =  9,
   number   =  8,
   pages    = "1735--1780",
   month    =  nov,
   year     =  1997,
   language = "en"
 }
 